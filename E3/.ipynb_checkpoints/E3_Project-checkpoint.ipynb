{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ mkdir -p ~/aiffel/news_crawler\n",
    "\n",
    "$ git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "\n",
    "$ cd Mecab-ko-for-Google-Colab\n",
    "\n",
    "$ bash install_mecab-ko_on_colab190912.sh\n",
    "\n",
    "\n",
    "\n",
    "$ pip install beautifulsoup4\n",
    "\n",
    "$ pip install newspaper3k\n",
    "\n",
    "$ pip install konlpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# 크롤러를 만들기 전 필요한 도구들을 임포트합니다.\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from konlpy.tag import Komoran\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "idx2word = {'101' : '경제', '102' : '사회', '103' : '생활/문화', '105' : 'IT/과학'}\n",
    "code_list = [101, 102, 103, 105]\n",
    "stopwords = ['한국경제','에서','으로','었','ㄴ다','며','ㅁ','로','ㄴ','ㄱ','로','라고''재', '배포', '고', '와','ㄹ','를', '에','는','은','을','했','에게','있','이','의','하','한','다','과','때문','할','수','무단','따른','및','금지','전재','경향신문','기자','는데','가','등','들','파이낸셜','저작','등','뉴스', 'ㄴ', '으며']\n",
    "print(len(stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 수, 카테고리, 날짜를 입력값으로 받습니다.\n",
    "def make_urllist(page_num, code, date): \n",
    "    urllist= []\n",
    "    for i in range(1, page_num + 1):\n",
    "        url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1='+str(code)+'&date='+str(date)+'&page='+str(i)   \n",
    "        news = requests.get(url)\n",
    "\n",
    "    # BeautifulSoup의 인스턴스 생성합니다. 파서는 html.parser를 사용합니다.\n",
    "        soup = BeautifulSoup(news.content, 'html.parser')\n",
    "\n",
    "    # CASE 1\n",
    "        news_list = soup.select('.newsflash_body .type06_headline li dl')\n",
    "    # CASE 2\n",
    "        news_list.extend(soup.select('.newsflash_body .type06 li dl'))\n",
    "        \n",
    "    # 각 뉴스로부터 a 태그인 <a href ='주소'> 에서 '주소'만을 가져옵니다.\n",
    "        for line in news_list:\n",
    "            urllist.append(line.a.get('href'))\n",
    "    return urllist\n",
    "\n",
    "\n",
    "\n",
    "#- 데이터프레임을 생성하는 함수입니다.\n",
    "def make_data(urllist, code):\n",
    "    text_list = []\n",
    "    for url in urllist:\n",
    "        article = Article(url, language='ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text_list.append(article.text)\n",
    "\n",
    "  #- 데이터프레임의 'news' 키 아래 파싱한 텍스트를 밸류로 붙여줍니다.\n",
    "    df = pd.DataFrame({'news': text_list})\n",
    "\n",
    "  #- 데이터프레임의 'code' 키 아래 한글 카테고리명을 붙여줍니다.\n",
    "    df['code'] = idx2word[str(code)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_total_data(page_num, code_list, date):\n",
    "    df = None\n",
    "    for code in code_list:\n",
    "        url_list = make_urllist(page_num, code, date)\n",
    "        df_temp = make_data(url_list, code)\n",
    "        print(str(code)+'번 코드에 대한 데이터를 만들었습니다.')\n",
    "\n",
    "        if df is not None:\n",
    "            df = pd.concat([df, df_temp])\n",
    "        else:\n",
    "            df = df_temp\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocessing(data):\n",
    "    text_data = []\n",
    "\n",
    "    for sentence in data:\n",
    "        temp_data = [] \n",
    "    #- 토큰화\n",
    "        temp_data = kkma.morphs(sentence)\n",
    "    #- 불용어 제거\n",
    "        temp_data = [word for word in temp_data if not word in stopwords] \n",
    "        text_data.append(temp_data)\n",
    "\n",
    "    text_data = list(map(' '.join, text_data))\n",
    "\n",
    "    return text_data\n",
    "\n",
    "def tfidf_vectorizer(data):\n",
    "    data_counts = count_vect.transform(data)\n",
    "    data_tfidf = tfidf_transformer.transform(data_counts)\n",
    "    return data_tfidf\n",
    "\n",
    "def preprocessing(data):\n",
    "    text_data = []\n",
    "\n",
    "    for sentence in data:\n",
    "        temp_data = []\n",
    "    #- 토큰화\n",
    "        temp_data = tokenizer.morphs(sentence) \n",
    "    #- 불용어 제거\n",
    "        temp_data = [word for word in temp_data if not word in stopwords] \n",
    "        text_data.append(temp_data)\n",
    "\n",
    "    text_data = list(map(' '.join, text_data))\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = make_total_data(100, code_list, 20200506)\n",
    "\n",
    "#print('뉴스 기사의 개수: ',len(df))\n",
    "\n",
    "#df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제로 크롤링을 할 수 없었기 때문에 \n",
    "## 노션에 업로드된 데이터 두 개를 병합하여 실험하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>기사 섹션 분류 안내\\n\\n기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>▶제21대 총선 실시간 개표 현황 및 결과 보기\\n\\n총선에서 여당이 다시 한 번 ...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[뉴욕=AP/뉴시스]지난 10일 뉴욕 증권거래소 건물에 미국 국기가 게양되어 있다....</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>부산지역 주유소에서 판매하는 기름값이 휘발유는 평균 1200원대, 경유는 1000원...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>담배업계가 소비자의 취향을 저격한 다양한 담배 신제품들을 잇달아 선보이고 있다.이전...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news code\n",
       "0  기사 섹션 분류 안내\\n\\n기사의 섹션 정보는 해당 언론사의 분류를 따르고 있습니다...   경제\n",
       "1  ▶제21대 총선 실시간 개표 현황 및 결과 보기\\n\\n총선에서 여당이 다시 한 번 ...   경제\n",
       "2  [뉴욕=AP/뉴시스]지난 10일 뉴욕 증권거래소 건물에 미국 국기가 게양되어 있다....   경제\n",
       "3  부산지역 주유소에서 판매하는 기름값이 휘발유는 평균 1200원대, 경유는 1000원...   경제\n",
       "4  담배업계가 소비자의 취향을 저격한 다양한 담배 신제품들을 잇달아 선보이고 있다.이전...   경제"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제로 크롤링을 할 수 없었기 때문에 \n",
    "# 노션에 업로드된 데이터 두 개를 병합하여 실험하였다. \n",
    "\n",
    "csv_path1 = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data2.csv\"\n",
    "df1 = pd.read_table(csv_path1, sep=',')\n",
    "\n",
    "csv_path2 = os.getenv(\"HOME\") + \"/aiffel/news_crawler/news_data.csv\"\n",
    "df2 = pd.read_table(csv_path2, sep=',')\n",
    "\n",
    "#두 데이터 합치기\n",
    "\n",
    "df = df1.append(df2, ignore_index=True)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news    0\n",
      "code    0\n",
      "dtype: int64\n",
      "    code  count\n",
      "0  IT/과학   1138\n",
      "1     경제    902\n",
      "2     사회   2221\n",
      "3  생활/문화   1866\n"
     ]
    }
   ],
   "source": [
    "# 정규 표현식을 이용해서 한글 외의 문자는 전부 제거합니다.\n",
    "df['news'] = df['news'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "df['news']\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# 중복된 샘플들을 제거합니다.\n",
    "df.drop_duplicates(subset=['news'], inplace=True)\n",
    "\n",
    "print(df.groupby('code').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Komoran()\n",
    "text_data = preprocessing(df['news'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 처리된 데이터들을 출력하며 필요없다고 판단되는 단어들을 불용어 사전에 추가하였다 ex) 'ㄴ다', '에서', 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스 기사의 개수 : 4595\n",
      "테스트용 뉴스 기사의 개수 :  1532\n",
      "훈련용 레이블의 개수 :  4595\n",
      "테스트용 레이블의 개수 :  1532\n"
     ]
    }
   ],
   "source": [
    "#print(text_data[4])\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data, df['code'], random_state = 0)\n",
    "\n",
    "print('훈련용 뉴스 기사의 개수 :', len(X_train))\n",
    "print('테스트용 뉴스 기사의 개수 : ', len(X_test))\n",
    "print('훈련용 레이블의 개수 : ', len(y_train))\n",
    "print('테스트용 레이블의 개수 : ', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- 단어의 수를 카운트하는 사이킷런의 카운트벡터라이저입니다.\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#- 카운트벡터라이저의 결과로부터 TF-IDF 결과를 얻습니다.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#더 나은 성능을 위해 다양한 분류기로 실험해보았고, 가장 좋은 성능을 보여준 SGD분류기로 실험을 하였다\n",
    "\n",
    "classifier = SGDClassifier()\n",
    "classifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IT/과학       0.81      0.85      0.83       271\n",
      "          경제       0.82      0.76      0.79       225\n",
      "          사회       0.88      0.87      0.87       559\n",
      "       생활/문화       0.80      0.82      0.81       477\n",
      "\n",
      "    accuracy                           0.83      1532\n",
      "   macro avg       0.83      0.82      0.83      1532\n",
      "weighted avg       0.83      0.83      0.83      1532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(tfidf_vectorizer(X_test))\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
